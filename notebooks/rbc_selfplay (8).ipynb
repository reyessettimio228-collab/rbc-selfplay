{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "sys.path.append(str(Path(\"..\").resolve()))"
      ],
      "metadata": {
        "id": "vzgtwYQZDu5J"
      },
      "id": "vzgtwYQZDu5J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5ab86fd6",
      "metadata": {
        "id": "5ab86fd6"
      },
      "source": [
        "# RBC AlphaZero-like Bot\n",
        "\n",
        "Reconnaissance Blind Chess (RBC) is a partially observable variant of chess in which players have perfect information about their own pieces but only limited observations of the opponent.\n",
        "\n",
        "This notebook describes a learning-based RBC agent inspired by the AlphaZero framework, combining neural network evaluation, search, and self-play training. The emphasis is on a clear and rule-compliant implementation that can be trained and evaluated end to end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDCD6msgmPye"
      },
      "source": [
        "##DEPENDENCIES\n"
      ],
      "id": "BDCD6msgmPye"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section lists the external libraries required to run the notebook.\n",
        "\n",
        "The implementation relies on standard numerical and deep learning tools for tensor computation and optimization, together with a chess engine library and the ReconChess framework to ensure correct handling of game rules and interaction between agents.\n",
        "\n",
        "These dependencies provide the basic infrastructure for representing game states, running self-play matches, training neural networks, and evaluating the resulting agent."
      ],
      "metadata": {
        "id": "NHjJ4oFO1dlU"
      },
      "id": "NHjJ4oFO1dlU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "bbc0f5bd-dc2c-4fbc-99fa-2ec3db4007fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xg1QTXxmfmE"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m136.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for chess (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip -q install python-chess reconchess\n"
      ],
      "id": "1Xg1QTXxmfmE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install required dependencies.\n",
        " - python-chess: standard chess representation and move generation\n",
        " - reconchess: official framework for Reconnaissance Blind Chess,\n",
        "   enabling rule-compliant gameplay against other bots\n"
      ],
      "metadata": {
        "id": "zQb0yzoCvC96"
      },
      "id": "zQb0yzoCvC96"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d35891e",
      "metadata": {
        "id": "4d35891e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import  random\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Tuple, List, Optional, Any\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import chess\n",
        "import reconchess\n",
        "from reconchess import Player, Color, Square\n",
        "import csv\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#REPRODUCIBILITY"
      ],
      "metadata": {
        "id": "oSqNk8hivIr1"
      },
      "id": "oSqNk8hivIr1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section lists the external libraries required to run the notebook.\n",
        "\n",
        "The implementation relies on standard numerical and deep learning tools for tensor computation and optimization, together with a chess engine library and the ReconChess framework to ensure correct handling of game rules and interaction between agents.\n",
        "\n",
        "These dependencies provide the basic infrastructure for representing game states, running self-play matches, training neural networks, and evaluating the resulting agent."
      ],
      "metadata": {
        "id": "waB4H1lA2MCE"
      },
      "id": "waB4H1lA2MCE"
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "sys.path.append(str(Path(\"..\").resolve()))\n",
        "\n",
        "from src.utils import set_seeds\n",
        "from src.config import DEVICE\n",
        "\n",
        "set_seeds(0)\n",
        "print(\"DEVICE:\", DEVICE)\n"
      ],
      "metadata": {
        "id": "AJoL3iUl_z8k"
      },
      "id": "AJoL3iUl_z8k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0339e161",
      "metadata": {
        "id": "0339e161"
      },
      "source": [
        "## ACTION ENCODING (20480 fixed policy head)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to use a fixed-size policy head, all possible chess moves are mapped to a discrete action space of fixed dimensionality.\n",
        "\n",
        "Each move is encoded as an index in a predefined action set of size 20,480, covering all standard chess moves, including promotions. This encoding allows the policy network to produce a fixed-length output independent of the current position.\n",
        "\n",
        "During play, only the subset of actions corresponding to legal moves provided by the game environment is considered, while the remaining entries are masked implicitly\n"
      ],
      "metadata": {
        "id": "p-oyeEH4tYN-"
      },
      "id": "p-oyeEH4tYN-"
    },
    {
      "cell_type": "code",
      "source": [
        "from src.encoding import (\n",
        "    PROMO_TO_ID,\n",
        "    ID_TO_PROMO,\n",
        "    POLICY_SIZE,\n",
        "    move_to_index,\n",
        "    index_to_move,   # se esiste nel tuo blocco\n",
        ")\n"
      ],
      "metadata": {
        "id": "AX5XGlpLCgZi"
      },
      "id": "AX5XGlpLCgZi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "419a7497",
      "metadata": {
        "id": "419a7497"
      },
      "source": [
        "## BELIEF TENSOR (7 channels) + SENSE UPDATE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uncertainty about the opponent’s pieces is represented through a per-square belief tensor with seven channels, corresponding to the six standard chess piece types plus an explicit EMPTY channel.\n",
        "\n",
        "For each board square, the belief tensor stores a probability distribution over these channels, normalized independently per square. This representation makes uncertainty explicit while remaining simple and easy to inspect.\n",
        "\n",
        "Sensing actions update the belief deterministically within the sensed 3×3 region: observed squares are set to the corresponding piece type or to EMPTY, while beliefs outside the sensed area remain unchanged. This local update rule provides a lightweight mechanism to incorporate new information without maintaining a full probabilistic game history."
      ],
      "metadata": {
        "id": "HkR-M-lU6HzZ"
      },
      "id": "HkR-M-lU6HzZ"
    },
    {
      "cell_type": "code",
      "source": [
        "from src.belief import (\n",
        "    normalize_over_channels,\n",
        "    init_belief_from_initial,\n",
        "    apply_sense_to_belief,\n",
        ")\n"
      ],
      "metadata": {
        "id": "uBRNQvlCFhya"
      },
      "id": "uBRNQvlCFhya",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "38d59bc3",
      "metadata": {
        "id": "38d59bc3"
      },
      "source": [
        "## SENSE SELECTION: entropy-max 3×3 center\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At each turn, the agent selects a sensing action by evaluating the uncertainty of the opponent’s belief distribution.\n",
        "\n",
        "For each allowed sensing square, the total entropy over the corresponding 3×3 region is computed, and the square that maximizes this value is selected. This heuristic prioritizes sensing actions that are expected to provide the largest reduction in uncertainty.\n",
        "\n",
        "The approach is purely information-driven and independent of the immediate move selection, making it simple, efficient, and consistent with the belief representation."
      ],
      "metadata": {
        "id": "dVDiuRfg6osh"
      },
      "id": "dVDiuRfg6osh"
    },
    {
      "cell_type": "code",
      "source": [
        "from src.sense import *\n"
      ],
      "metadata": {
        "id": "RgbyZJrrKfAY"
      },
      "id": "RgbyZJrrKfAY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "61570cb2",
      "metadata": {
        "id": "61570cb2"
      },
      "source": [
        "## GREEDY DETERMINIZATION FROM BELIEF + remaining opponent inventory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To enable fast planning with standard chess move generation, the opponent’s hidden position is approximated by constructing a single fully specified “determinized” board state from the belief tensor.\n",
        "\n",
        "For each opponent piece type, the algorithm places the remaining pieces on the highest-probability squares according to the belief distribution, while respecting already occupied squares (including all known own pieces). A simple opponent inventory is maintained to ensure that the determinized position contains a consistent number of pieces of each type.\n",
        "\n",
        "The resulting determinized board is used only as a hypothesis for search and evaluation; it provides a concrete state on which legal moves can be checked and simulated efficiently."
      ],
      "metadata": {
        "id": "9tH92ZreoHN5"
      },
      "id": "9tH92ZreoHN5"
    },
    {
      "cell_type": "code",
      "source": [
        "from src.determinize import *\n"
      ],
      "metadata": {
        "id": "ak6qiWNYLOSs"
      },
      "id": "ak6qiWNYLOSs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "43551a8f",
      "metadata": {
        "id": "43551a8f"
      },
      "source": [
        "## ENCODER (own pieces + belief + small metadata) → 15×8×8\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The neural network input is a stack of 2D feature planes with fixed spatial resolution (8×8), producing a tensor of shape 15×8×8.\n",
        "\n",
        "The encoding includes: (i) six binary planes for the agent’s own pieces (one per piece type), (ii) the seven-channel opponent belief tensor, and (iii) a small set of global metadata planes (side to move and a normalized move counter).\n",
        "\n",
        "This representation keeps the input compact while preserving the spatial structure of the board, allowing convolutional layers to exploit local patterns and piece configurations."
      ],
      "metadata": {
        "id": "ueWC4vap-xxi"
      },
      "id": "ueWC4vap-xxi"
    },
    {
      "cell_type": "code",
      "source": [
        "from src.encoder import *\n"
      ],
      "metadata": {
        "id": "XK82ilGrN3z3"
      },
      "id": "XK82ilGrN3z3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "df6d0b57",
      "metadata": {
        "id": "df6d0b57"
      },
      "source": [
        "## SMALL POLICY/VALUE NET\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The agent uses a lightweight convolutional neural network with a shared trunk and two output heads: a policy head and a value head.\n",
        "\n",
        "The policy head produces logits over the fixed 20,480-action encoding, which are later restricted to the legal moves available in the current position. The value head outputs a single scalar estimating the expected game outcome from the current player’s perspective.\n",
        "\n",
        "The network is intentionally small to keep self-play and training fast while still capturing the spatial structure of the board representation."
      ],
      "metadata": {
        "id": "jRLjtGDeAq4j"
      },
      "id": "jRLjtGDeAq4j"
    },
    {
      "cell_type": "code",
      "source": [
        "from src.model import FastPolicyValueNet\n"
      ],
      "metadata": {
        "id": "14a8Si6DOUT5"
      },
      "id": "14a8Si6DOUT5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "863543ae",
      "metadata": {
        "id": "863543ae"
      },
      "source": [
        "## ROOT-only PUCT (search on determinization, choose among provided legal actions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Move selection is performed using a lightweight, root-only PUCT search guided by the network’s policy priors.\n",
        "\n",
        "The search is run on the determinized board hypothesis and considers only the move actions provided by the ReconChess environment for the current turn. Each simulation selects the move that maximizes a PUCT score combining an exploitation term (estimated value) and an exploration term weighted by the network prior.\n",
        "\n",
        "The final move is chosen from the resulting visit counts, producing a search-improved policy target that is also reused during training."
      ],
      "metadata": {
        "id": "yrfPPWZ7CmFx"
      },
      "id": "yrfPPWZ7CmFx"
    },
    {
      "cell_type": "code",
      "source": [
        "from src.search import *\n"
      ],
      "metadata": {
        "id": "qCjkxgVaRGvI"
      },
      "id": "qCjkxgVaRGvI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4e26065a",
      "metadata": {
        "id": "4e26065a"
      },
      "source": [
        "## THE **RECONCHESS PLAYER** (FAST, rule-compliant)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b01afc4",
      "metadata": {
        "id": "2b01afc4"
      },
      "source": [
        "The agent is implemented as a ReconChess Player, fully compliant with the game’s interface and rules.\n",
        "\n",
        "All game interactions—including sensing, move selection, belief updates, and board state tracking—are handled through the standard ReconChess callbacks. This ensures that the agent can play against other bots without relying on privileged information or modified game mechanics. The focus is on robustness and correct interaction rather than maximal performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from src.player import *\n"
      ],
      "metadata": {
        "id": "6jEPD7i9TYih"
      },
      "id": "6jEPD7i9TYih",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cd218048",
      "metadata": {
        "id": "cd218048"
      },
      "source": [
        "## LOCAL MATCH HARNESS (ReconChess) — smoke test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A local match harness is used to run short games against a baseline opponent, serving as an end-to-end smoke test for rule compliance and framework integration."
      ],
      "metadata": {
        "id": "atnVMf10-rtL"
      },
      "id": "atnVMf10-rtL"
    },
    {
      "cell_type": "code",
      "source": [
        "# See play_local.py for local smoke testing\n"
      ],
      "metadata": {
        "id": "MPsoUyBtVcVC"
      },
      "id": "MPsoUyBtVcVC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0c48c030",
      "metadata": {
        "id": "0c48c030"
      },
      "source": [
        "### Run smoke test (uncomment)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c7230b9",
      "metadata": {
        "id": "9c7230b9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "1e6791e3",
      "metadata": {
        "id": "1e6791e3"
      },
      "source": [
        "## RUN ALL CHECKS (fast sanity gate)\n",
        "These checks are meant to fail fast if something is inconsistent. If they pass, the agent is generally safe to run in local matches and self-play\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f18c59af",
      "metadata": {
        "id": "f18c59af"
      },
      "source": [
        "# SELF-PLAY + TRAINING (FAST)\n",
        "\n",
        "This section makes the bot trainable with minimal extra code:\n",
        "\n",
        "1) ReplayBuffer in RAM (FAST)\n",
        "2) Training step: KL(policy) + MSE(value)\n",
        "3) Self-play game generator (ReconChess local runner)\n",
        "4) Iterative loop: self-play → train → eval → checkpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fast sanity gate moved to sanity_checks.py\n"
      ],
      "metadata": {
        "id": "ZHB_pohiZWKR"
      },
      "id": "ZHB_pohiZWKR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "986d1186",
      "metadata": {
        "id": "986d1186"
      },
      "source": [
        "## ReplayBuffer (RAM) + Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self-play generates training samples over time, so the implementation stores them in a replay buffer kept in RAM.\n",
        "\n",
        "The buffer collects tuples (X,π,z), where X is the encoded state, π is the search-improved policy target, and z is the final game outcome from the player’s perspective. A maximum capacity is enforced by discarding the oldest samples, keeping the dataset bounded and biased toward more recent experience.\n",
        "\n",
        "A lightweight PyTorch Dataset wrapper exposes the buffer in a format suitable for batching with a DataLoader, enabling standard supervised updates of the policy and value network."
      ],
      "metadata": {
        "id": "UjXDQ_MAA-DD"
      },
      "id": "UjXDQ_MAA-DD"
    },
    {
      "cell_type": "code",
      "source": [
        "from src.replay import *\n"
      ],
      "metadata": {
        "id": "M9xHxO2EaZ2x"
      },
      "id": "M9xHxO2EaZ2x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5bb60965",
      "metadata": {
        "id": "5bb60965"
      },
      "source": [
        "## Training step (KL policy + MSE value)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Network parameters are updated using supervised learning on batches sampled from the replay buffer.\n",
        "\n",
        "The policy head is trained by minimizing the Kullback–Leibler divergence between the network’s predicted action distribution and the search-derived policy target. In parallel, the value head is trained using a mean squared error loss against the final game outcome.\n",
        "\n",
        "The two losses are combined into a single objective and optimized using standard gradient-based methods, with gradient clipping applied for stability."
      ],
      "metadata": {
        "id": "gGBXuu9pBqxu"
      },
      "id": "gGBXuu9pBqxu"
    },
    {
      "cell_type": "code",
      "source": [
        "from src.train_loop import train_steps\n"
      ],
      "metadata": {
        "id": "O-PzEXbIlc_r"
      },
      "id": "O-PzEXbIlc_r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1015be23",
      "metadata": {
        "id": "1015be23"
      },
      "source": [
        "## Checkpoint helpers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To support long-running experiments and allow training to be resumed across sessions, helper functions are provided to save and load model checkpoints.\n",
        "\n",
        "Each checkpoint stores the network parameters, optimizer state, and basic training metadata, ensuring that training can be restarted consistently without loss of information."
      ],
      "metadata": {
        "id": "CMT1C2IeCxj_"
      },
      "id": "CMT1C2IeCxj_"
    },
    {
      "cell_type": "code",
      "source": [
        "from src.checkpoint import *\n"
      ],
      "metadata": {
        "id": "3OA8TpQOq8qC"
      },
      "id": "3OA8TpQOq8qC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Use src.checkpoint in notebook\n"
      ],
      "metadata": {
        "id": "IlqQBYcEq8ad"
      },
      "id": "IlqQBYcEq8ad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1d996c0e",
      "metadata": {
        "id": "1d996c0e"
      },
      "source": [
        "## One self-play game (ReconChess) → (X, P, Z)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A single self-play game is executed by running two instances of the same ReconChess player against each other using the local game runner.\n",
        "\n",
        "During the game, each player records training samples (X,π) at decision time, where X is the encoded state and π is derived from the search visit counts. After the game ends, the final outcome is converted into a value target z and assigned to all samples collected by each player.\n",
        "\n",
        "The resulting lists (X,P,Z) provide one complete episode of training data that can be appended to the replay buffer."
      ],
      "metadata": {
        "id": "DeyM690GEOG7"
      },
      "id": "DeyM690GEOG7"
    },
    {
      "cell_type": "markdown",
      "id": "0c0b3093",
      "metadata": {
        "id": "0c0b3093"
      },
      "source": [
        "## Self-play → train → eval loop (FAST)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from src.selfplay import *\n"
      ],
      "metadata": {
        "id": "qd5SZmHVtdG_"
      },
      "id": "qd5SZmHVtdG_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main training loop alternates between data generation and network updates.\n",
        "\n",
        "At each iteration, a small batch of self-play games is generated to produce new (X,π,z) samples, which are appended to the replay buffer. The policy/value network is then updated for a fixed number of gradient steps using mini-batches sampled from the buffer.\n",
        "\n",
        "After training, the current model is evaluated in a short match series against a simple baseline opponent to provide a quick progress signal, and a checkpoint plus a CSV log entry are saved for later inspection"
      ],
      "metadata": {
        "id": "bDqwYyy8Eyx1"
      },
      "id": "bDqwYyy8Eyx1"
    },
    {
      "cell_type": "code",
      "source": [
        "from src.selfplay import *\n"
      ],
      "metadata": {
        "id": "6Vtcnkx-uCpf"
      },
      "id": "6Vtcnkx-uCpf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2cfea0e2",
      "metadata": {
        "id": "2cfea0e2"
      },
      "source": [
        "### Run a small self-play training (start tiny)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95f0a1bc",
      "metadata": {
        "id": "95f0a1bc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "8e7e62a6",
      "metadata": {
        "id": "8e7e62a6"
      },
      "source": [
        "## Results plots\n",
        "After training, plot loss and winrate vs random.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from src.plots import *\n"
      ],
      "metadata": {
        "id": "lediD5K8vQYP"
      },
      "id": "lediD5K8vQYP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook provides a complete and executable reference implementation of a learning-based RBC agent, suitable for experimentation and further extensions."
      ],
      "metadata": {
        "id": "884Ab5AwHbKs"
      },
      "id": "884Ab5AwHbKs"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}