{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5ab86fd6",
      "metadata": {
        "id": "5ab86fd6"
      },
      "source": [
        "# RBC AlphaZero-like Bot\n",
        "\n",
        "Reconnaissance Blind Chess (RBC) is a partially observable variant of chess in which players have perfect information about their own pieces but only limited observations of the opponent.\n",
        "\n",
        "This notebook describes a learning-based RBC agent inspired by the AlphaZero framework, combining neural network evaluation, search, and self-play training. The emphasis is on a clear and rule-compliant implementation that can be trained and evaluated end to end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDCD6msgmPye"
      },
      "source": [
        "##DEPENDENCIES\n"
      ],
      "id": "BDCD6msgmPye"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section lists the external libraries required to run the notebook.\n",
        "\n",
        "The implementation relies on standard numerical and deep learning tools for tensor computation and optimization, together with a chess engine library and the ReconChess framework to ensure correct handling of game rules and interaction between agents.\n",
        "\n",
        "These dependencies provide the basic infrastructure for representing game states, running self-play matches, training neural networks, and evaluating the resulting agent."
      ],
      "metadata": {
        "id": "NHjJ4oFO1dlU"
      },
      "id": "NHjJ4oFO1dlU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "bbc0f5bd-dc2c-4fbc-99fa-2ec3db4007fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xg1QTXxmfmE"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m136.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for chess (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip -q install python-chess reconchess\n"
      ],
      "id": "1Xg1QTXxmfmE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install required dependencies.\n",
        " - python-chess: standard chess representation and move generation\n",
        " - reconchess: official framework for Reconnaissance Blind Chess,\n",
        "   enabling rule-compliant gameplay against other bots\n"
      ],
      "metadata": {
        "id": "zQb0yzoCvC96"
      },
      "id": "zQb0yzoCvC96"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d35891e",
      "metadata": {
        "id": "4d35891e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import  random\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Tuple, List, Optional, Any\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import chess\n",
        "import reconchess\n",
        "from reconchess import Player, Color, Square\n",
        "import csv\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#REPRODUCIBILITY"
      ],
      "metadata": {
        "id": "oSqNk8hivIr1"
      },
      "id": "oSqNk8hivIr1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section lists the external libraries required to run the notebook.\n",
        "\n",
        "The implementation relies on standard numerical and deep learning tools for tensor computation and optimization, together with a chess engine library and the ReconChess framework to ensure correct handling of game rules and interaction between agents.\n",
        "\n",
        "These dependencies provide the basic infrastructure for representing game states, running self-play matches, training neural networks, and evaluating the resulting agent."
      ],
      "metadata": {
        "id": "waB4H1lA2MCE"
      },
      "id": "waB4H1lA2MCE"
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "sys.path.append(str(Path(\"..\").resolve()))\n",
        "\n",
        "from src.utils import set_seeds\n",
        "from src.config import DEVICE\n",
        "\n",
        "set_seeds(0)\n",
        "print(\"DEVICE:\", DEVICE)\n"
      ],
      "metadata": {
        "id": "AJoL3iUl_z8k"
      },
      "id": "AJoL3iUl_z8k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0339e161",
      "metadata": {
        "id": "0339e161"
      },
      "source": [
        "## ACTION ENCODING (20480 fixed policy head)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to use a fixed-size policy head, all possible chess moves are mapped to a discrete action space of fixed dimensionality.\n",
        "\n",
        "Each move is encoded as an index in a predefined action set of size 20,480, covering all standard chess moves, including promotions. This encoding allows the policy network to produce a fixed-length output independent of the current position.\n",
        "\n",
        "During play, only the subset of actions corresponding to legal moves provided by the game environment is considered, while the remaining entries are masked implicitly\n"
      ],
      "metadata": {
        "id": "p-oyeEH4tYN-"
      },
      "id": "p-oyeEH4tYN-"
    },
    {
      "cell_type": "code",
      "source": [
        "from src.encoding import (\n",
        "    PROMO_TO_ID,\n",
        "    ID_TO_PROMO,\n",
        "    POLICY_SIZE,\n",
        "    move_to_index,\n",
        "    index_to_move,   # se esiste nel tuo blocco\n",
        ")\n"
      ],
      "metadata": {
        "id": "AX5XGlpLCgZi"
      },
      "id": "AX5XGlpLCgZi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "419a7497",
      "metadata": {
        "id": "419a7497"
      },
      "source": [
        "## BELIEF TENSOR (7 channels) + SENSE UPDATE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uncertainty about the opponent’s pieces is represented through a per-square belief tensor with seven channels, corresponding to the six standard chess piece types plus an explicit EMPTY channel.\n",
        "\n",
        "For each board square, the belief tensor stores a probability distribution over these channels, normalized independently per square. This representation makes uncertainty explicit while remaining simple and easy to inspect.\n",
        "\n",
        "Sensing actions update the belief deterministically within the sensed 3×3 region: observed squares are set to the corresponding piece type or to EMPTY, while beliefs outside the sensed area remain unchanged. This local update rule provides a lightweight mechanism to incorporate new information without maintaining a full probabilistic game history."
      ],
      "metadata": {
        "id": "HkR-M-lU6HzZ"
      },
      "id": "HkR-M-lU6HzZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Belief tensor: definitions"
      ],
      "metadata": {
        "id": "2ycfMtekw_jD"
      },
      "id": "2ycfMtekw_jD"
    },
    {
      "cell_type": "code",
      "source": [
        "PIECE_TYPES_6 = [\"P\", \"N\", \"B\", \"R\", \"Q\", \"K\"]\n",
        "EMPTY = \"EMPTY\"\n",
        "CHANNELS_7 = PIECE_TYPES_6 + [EMPTY]\n",
        "C_BELIEF = 7\n",
        "CH2I = {ch: i for i, ch in enumerate(CHANNELS_7)}\n",
        "START_COUNTS = {\"P\": 8, \"N\": 2, \"B\": 2, \"R\": 2, \"Q\": 1, \"K\": 1}\n"
      ],
      "metadata": {
        "id": "bzkirVoJwXOc"
      },
      "id": "bzkirVoJwXOc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization over channels (to make it a true distribution)"
      ],
      "metadata": {
        "id": "nSIUrv64xCjD"
      },
      "id": "nSIUrv64xCjD"
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_over_channels(B: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
        "    s = B.sum(dim=0, keepdim=True).clamp_min(eps)\n",
        "    return B / s\n"
      ],
      "metadata": {
        "id": "d2odcQhBwiw1"
      },
      "id": "d2odcQhBwiw1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def piece_symbol_upper(pc: chess.Piece) -> str:\n",
        "    return pc.symbol().upper()"
      ],
      "metadata": {
        "id": "6MF457gHwmD_"
      },
      "id": "6MF457gHwmD_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Belief's initialization from the standard board position"
      ],
      "metadata": {
        "id": "cDug43ZOxk4r"
      },
      "id": "cDug43ZOxk4r"
    },
    {
      "cell_type": "code",
      "source": [
        "def init_belief_from_initial(my_color: chess.Color) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    RBC starts from the standard chess initial position.\n",
        "    We initialize the opponent belief with a strong prior: probability 1 on the opponent’s\n",
        "    initial piece placement, and EMPTY elsewhere.\n",
        "    This is a reasonable baseline because the starting configuration is known in RBC;\n",
        "    only after the game starts the opponent becomes hidden.\n",
        "    \"\"\"\n",
        "    B = torch.zeros((C_BELIEF, 8, 8), dtype=torch.float32)\n",
        "    B[CH2I[EMPTY]].fill_(1.0)\n",
        "    board = chess.Board()\n",
        "    opp_color = not my_color\n",
        "    for sq, pc in board.piece_map().items():\n",
        "        if pc.color != opp_color:\n",
        "            continue\n",
        "        r = chess.square_rank(sq)\n",
        "        f = chess.square_file(sq)\n",
        "        sym = piece_symbol_upper(pc)\n",
        "        B[:, r, f] = 0.0\n",
        "        B[CH2I[sym], r, f] = 1.0\n",
        "    return normalize_over_channels(B)\n"
      ],
      "metadata": {
        "id": "TbqQwJa5w0Nn"
      },
      "id": "TbqQwJa5w0Nn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Updates the opponent belief tensor after a sensing action."
      ],
      "metadata": {
        "id": "wP_S90fFyEvC"
      },
      "id": "wP_S90fFyEvC"
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_sense_to_belief(\n",
        "    B: torch.Tensor,\n",
        "    sense_result: List[Tuple[Square, Optional[chess.Piece]]],\n",
        "    my_color: chess.Color\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"Update opponent belief using ReconChess sense results (copy-based, not in-place).\"\"\"\n",
        "    B2 = B.clone()\n",
        "    for sq, pc in sense_result:\n",
        "        r = chess.square_rank(sq)\n",
        "        f = chess.square_file(sq)\n",
        "        B2[:, r, f] = 0.0\n",
        "        if pc is None or pc.color == my_color:\n",
        "            B2[CH2I[EMPTY], r, f] = 1.0\n",
        "        else:\n",
        "            sym = piece_symbol_upper(pc)\n",
        "            B2[CH2I[sym], r, f] = 1.0\n",
        "    return normalize_over_channels(B2)"
      ],
      "metadata": {
        "id": "nIjIiN_V2x4T"
      },
      "id": "nIjIiN_V2x4T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "38d59bc3",
      "metadata": {
        "id": "38d59bc3"
      },
      "source": [
        "## SENSE SELECTION: entropy-max 3×3 center\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At each turn, the agent selects a sensing action by evaluating the uncertainty of the opponent’s belief distribution.\n",
        "\n",
        "For each allowed sensing square, the total entropy over the corresponding 3×3 region is computed, and the square that maximizes this value is selected. This heuristic prioritizes sensing actions that are expected to provide the largest reduction in uncertainty.\n",
        "\n",
        "The approach is purely information-driven and independent of the immediate move selection, making it simple, efficient, and consistent with the belief representation."
      ],
      "metadata": {
        "id": "dVDiuRfg6osh"
      },
      "id": "dVDiuRfg6osh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dbaac87",
      "metadata": {
        "id": "8dbaac87"
      },
      "outputs": [],
      "source": [
        "SENSE_OFFSETS = [(dr, df) for dr in (-1,0,1) for df in (-1,0,1)]\n",
        "def square_entropy(p: torch.Tensor, eps: float = 1e-8) -> float:\n",
        "    p = p.clamp_min(eps)\n",
        "    return float(-(p * p.log()).sum().item())\n",
        "def choose_sense_square_entropy(B: torch.Tensor, sense_actions: List[Square]) -> Square:\n",
        "    \"\"\"Choose among *allowed* sense_actions provided by ReconChess.\"\"\"\n",
        "    best_sq = sense_actions[0]\n",
        "    best_e = -1e18\n",
        "    for sq in sense_actions:\n",
        "        r0 = chess.square_rank(sq)\n",
        "        f0 = chess.square_file(sq)\n",
        "        tot = 0.0\n",
        "        for dr, df in SENSE_OFFSETS:\n",
        "            r = r0 + dr; f = f0 + df\n",
        "            if 0 <= r < 8 and 0 <= f < 8:\n",
        "                tot += square_entropy(B[:, r, f])\n",
        "        if tot > best_e:\n",
        "            best_e = tot\n",
        "            best_sq = sq\n",
        "    return best_sq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61570cb2",
      "metadata": {
        "id": "61570cb2"
      },
      "source": [
        "## GREEDY DETERMINIZATION FROM BELIEF + remaining opponent inventory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To enable fast planning with standard chess move generation, the opponent’s hidden position is approximated by constructing a single fully specified “determinized” board state from the belief tensor.\n",
        "\n",
        "For each opponent piece type, the algorithm places the remaining pieces on the highest-probability squares according to the belief distribution, while respecting already occupied squares (including all known own pieces). A simple opponent inventory is maintained to ensure that the determinized position contains a consistent number of pieces of each type.\n",
        "\n",
        "The resulting determinized board is used only as a hypothesis for search and evaluation; it provides a concrete state on which legal moves can be checked and simulated efficiently."
      ],
      "metadata": {
        "id": "9tH92ZreoHN5"
      },
      "id": "9tH92ZreoHN5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determinize the hidden opponent position from belief."
      ],
      "metadata": {
        "id": "HA_7wkXIegf7"
      },
      "id": "HA_7wkXIegf7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2b55805",
      "metadata": {
        "id": "b2b55805"
      },
      "outputs": [],
      "source": [
        "def greedy_determinize_opponent(\n",
        "    B: torch.Tensor,\n",
        "    own_board: chess.Board,\n",
        "    my_color: chess.Color,\n",
        "    opp_counts: Dict[str, int],\n",
        ") -> chess.Board:\n",
        "    det = chess.Board(None)\n",
        "    det.clear()\n",
        "    for sq, pc in own_board.piece_map().items():\n",
        "        det.set_piece_at(sq, pc)\n",
        "    occupied = set(own_board.piece_map().keys())\n",
        "    opp_color = not my_color\n",
        "    for sym in PIECE_TYPES_6:\n",
        "        k = int(opp_counts.get(sym, 0))\n",
        "        if k <= 0:\n",
        "            continue\n",
        "        probs = []\n",
        "        ch = CH2I[sym]\n",
        "        for sq in chess.SQUARES:\n",
        "            if sq in occupied:\n",
        "                continue\n",
        "            r = chess.square_rank(sq)\n",
        "            f = chess.square_file(sq)\n",
        "            probs.append((float(B[ch, r, f].item()), sq))\n",
        "        probs.sort(reverse=True, key=lambda x: x[0])\n",
        "        placed = 0\n",
        "        for _, sq in probs:\n",
        "            if sq in occupied:\n",
        "                continue\n",
        "            psym = sym.lower() if opp_color == chess.BLACK else sym\n",
        "            det.set_piece_at(sq, chess.Piece.from_symbol(psym))\n",
        "            occupied.add(sq)\n",
        "            placed += 1\n",
        "            if placed >= k:\n",
        "                break\n",
        "    det.turn = own_board.turn\n",
        "    det.castling_rights = own_board.castling_rights\n",
        "    return det"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e565bc66",
      "metadata": {
        "id": "e565bc66"
      },
      "outputs": [],
      "source": [
        "def apply_taken_move_to_own_board(own_board: chess.Board, mv: chess.Move, my_color: chess.Color) -> None:\n",
        "    pc = own_board.piece_at(mv.from_square)\n",
        "    if pc is None or pc.color != my_color:\n",
        "        return\n",
        "    own_board.remove_piece_at(mv.from_square)\n",
        "    if mv.promotion is not None and pc.piece_type == chess.PAWN:\n",
        "        pc = chess.Piece(mv.promotion, my_color)\n",
        "    own_board.set_piece_at(mv.to_square, pc)\n",
        "    if pc.piece_type == chess.KING:\n",
        "        f_from = chess.square_file(mv.from_square)\n",
        "        f_to = chess.square_file(mv.to_square)\n",
        "        r_rank = chess.square_rank(mv.from_square)\n",
        "        if abs(f_to - f_from) == 2:\n",
        "            if f_to > f_from:\n",
        "                rook_from = chess.square(7, r_rank)\n",
        "                rook_to = chess.square(5, r_rank)\n",
        "            else:\n",
        "                rook_from = chess.square(0, r_rank)\n",
        "                rook_to = chess.square(3, r_rank)\n",
        "            rook = own_board.piece_at(rook_from)\n",
        "            if rook is not None and rook.color == my_color and rook.piece_type == chess.ROOK:\n",
        "                own_board.remove_piece_at(rook_from)\n",
        "                own_board.set_piece_at(rook_to, rook)\n",
        "    own_board.turn = not own_board.turn\n",
        "    if my_color == chess.BLACK:\n",
        "        own_board.fullmove_number += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43551a8f",
      "metadata": {
        "id": "43551a8f"
      },
      "source": [
        "## ENCODER (own pieces + belief + small metadata) → 15×8×8\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The neural network input is a stack of 2D feature planes with fixed spatial resolution (8×8), producing a tensor of shape 15×8×8.\n",
        "\n",
        "The encoding includes: (i) six binary planes for the agent’s own pieces (one per piece type), (ii) the seven-channel opponent belief tensor, and (iii) a small set of global metadata planes (side to move and a normalized move counter).\n",
        "\n",
        "This representation keeps the input compact while preserving the spatial structure of the board, allowing convolutional layers to exploit local patterns and piece configurations."
      ],
      "metadata": {
        "id": "ueWC4vap-xxi"
      },
      "id": "ueWC4vap-xxi"
    },
    {
      "cell_type": "code",
      "source": [
        "PIECE_ORDER = [chess.PAWN, chess.KNIGHT, chess.BISHOP, chess.ROOK, chess.QUEEN, chess.KING]\n",
        "def board_to_own_planes(board: chess.Board, my_color: chess.Color) -> torch.Tensor:\n",
        "    X = torch.zeros((6, 8, 8), dtype=torch.float32)\n",
        "    for sq, pc in board.piece_map().items():\n",
        "        if pc.color != my_color:\n",
        "            continue\n",
        "        r = chess.square_rank(sq)\n",
        "        f = chess.square_file(sq)\n",
        "        i = PIECE_ORDER.index(pc.piece_type)\n",
        "        X[i, r, f] = 1.0\n",
        "    return X\n",
        "def metadata_planes(board: chess.Board, my_color: chess.Color) -> torch.Tensor:\n",
        "    turn_plane = torch.full((1,8,8), 1.0 if board.turn == my_color else 0.0, dtype=torch.float32)\n",
        "    ply = min(board.fullmove_number * 2, 200) / 200.0\n",
        "    ply_plane = torch.full((1,8,8), float(ply), dtype=torch.float32)\n",
        "    return torch.cat([turn_plane, ply_plane], dim=0)\n",
        "def encode_state(own_board: chess.Board, my_color: chess.Color, B: torch.Tensor) -> torch.Tensor:\n",
        "    own = board_to_own_planes(own_board, my_color)\n",
        "    meta = metadata_planes(own_board, my_color)\n",
        "    return torch.cat([own, B, meta], dim=0)\n"
      ],
      "metadata": {
        "id": "fv3s3oqvJh7H"
      },
      "id": "fv3s3oqvJh7H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "df6d0b57",
      "metadata": {
        "id": "df6d0b57"
      },
      "source": [
        "## SMALL POLICY/VALUE NET\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The agent uses a lightweight convolutional neural network with a shared trunk and two output heads: a policy head and a value head.\n",
        "\n",
        "The policy head produces logits over the fixed 20,480-action encoding, which are later restricted to the legal moves available in the current position. The value head outputs a single scalar estimating the expected game outcome from the current player’s perspective.\n",
        "\n",
        "The network is intentionally small to keep self-play and training fast while still capturing the spatial structure of the board representation."
      ],
      "metadata": {
        "id": "jRLjtGDeAq4j"
      },
      "id": "jRLjtGDeAq4j"
    },
    {
      "cell_type": "code",
      "source": [
        "class FastPolicyValueNet(nn.Module):\n",
        "    def __init__(self, in_ch: int = 15, trunk: int = 64):\n",
        "        super().__init__()\n",
        "        self.trunk = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, trunk, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(trunk, trunk, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.pol = nn.Sequential(nn.Conv2d(trunk, 32, 1), nn.ReLU())\n",
        "        self.pol_fc = nn.Linear(32*8*8, POLICY_SIZE)\n",
        "        self.val = nn.Sequential(nn.Conv2d(trunk, 16, 1), nn.ReLU())\n",
        "        self.val_fc1 = nn.Linear(16*8*8, 64)\n",
        "        self.val_fc2 = nn.Linear(64, 1)\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        h = self.trunk(x)\n",
        "        logits = self.pol_fc(self.pol(h).flatten(1))\n",
        "        v = torch.tanh(self.val_fc2(F.relu(self.val_fc1(self.val(h).flatten(1))))).squeeze(-1)\n",
        "        return logits, v\n"
      ],
      "metadata": {
        "id": "a8x3KW5YO9_M"
      },
      "id": "a8x3KW5YO9_M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "863543ae",
      "metadata": {
        "id": "863543ae"
      },
      "source": [
        "## ROOT-only PUCT (search on determinization, choose among provided legal actions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Move selection is performed using a lightweight, root-only PUCT search guided by the network’s policy priors.\n",
        "\n",
        "The search is run on the determinized board hypothesis and considers only the move actions provided by the ReconChess environment for the current turn. Each simulation selects the move that maximizes a PUCT score combining an exploitation term (estimated value) and an exploration term weighted by the network prior.\n",
        "\n",
        "The final move is chosen from the resulting visit counts, producing a search-improved policy target that is also reused during training."
      ],
      "metadata": {
        "id": "yrfPPWZ7CmFx"
      },
      "id": "yrfPPWZ7CmFx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d422192b",
      "metadata": {
        "id": "d422192b"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def nn_priors_and_value(model: nn.Module, x: torch.Tensor, legal_moves: List[chess.Move], device: str) -> Tuple[Dict[int,float], float]:\n",
        "    model.eval()\n",
        "    xb = x.unsqueeze(0).to(device)\n",
        "    logits, v = model(xb)\n",
        "    logits = logits[0].cpu()\n",
        "    v = float(v.item())\n",
        "    idxs = [move_to_index(m) for m in legal_moves]\n",
        "    l = logits[idxs]\n",
        "    p = torch.softmax(l, dim=0).numpy()\n",
        "    pri = {idxs[i]: float(p[i]) for i in range(len(idxs))}\n",
        "    return pri, v\n",
        "@torch.no_grad()\n",
        "def puct_root(\n",
        "    model: nn.Module,\n",
        "    x_root: torch.Tensor,\n",
        "    det_board: chess.Board,\n",
        "    move_actions: List[chess.Move],\n",
        "    sims: int = 80,\n",
        "    c_puct: float = 1.5,\n",
        "    device: str = \"cpu\",\n",
        ") -> Dict[int,int]:\n",
        "    if not move_actions:\n",
        "        return {}\n",
        "    priors, _ = nn_priors_and_value(model, x_root, move_actions, device=device)\n",
        "    N = 0\n",
        "    N_a: Dict[int,int] = {move_to_index(m): 0 for m in move_actions}\n",
        "    W_a: Dict[int,float] = {move_to_index(m): 0.0 for m in move_actions}\n",
        "    def Q(a):\n",
        "        n = N_a[a]\n",
        "        return 0.0 if n == 0 else W_a[a] / n\n",
        "    for _ in range(sims):\n",
        "        N += 1\n",
        "        best_a = None\n",
        "        best_s = -1e18\n",
        "        for m in move_actions:\n",
        "            a = move_to_index(m)\n",
        "            u = c_puct * priors.get(a, 0.0) * math.sqrt(N) / (1 + N_a[a])\n",
        "            s = Q(a) + u\n",
        "            if s > best_s:\n",
        "                best_s = s\n",
        "                best_a = a\n",
        "        mv = index_to_move(best_a)\n",
        "        b2 = det_board.copy()\n",
        "        if mv not in b2.legal_moves:\n",
        "            v_leaf = -1.0\n",
        "        else:\n",
        "            b2.push(mv)\n",
        "            own2 = chess.Board(None); own2.clear()\n",
        "            for sq, pc in b2.piece_map().items():\n",
        "                if pc.color == det_board.turn:\n",
        "                    pass\n",
        "            _, v = model(x_root.unsqueeze(0).to(device))\n",
        "            v_leaf = float(v.item())\n",
        "        N_a[best_a] += 1\n",
        "        W_a[best_a] += float(v_leaf)\n",
        "    return N_a\n",
        "def visits_to_policy_target(N_a: Dict[int,int]) -> np.ndarray:\n",
        "    P = np.zeros((POLICY_SIZE,), dtype=np.float32)\n",
        "    if not N_a:\n",
        "        return P\n",
        "    total = sum(N_a.values())\n",
        "    for a, n in N_a.items():\n",
        "        P[a] = n / max(1, total)\n",
        "    return P"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e26065a",
      "metadata": {
        "id": "4e26065a"
      },
      "source": [
        "## THE **RECONCHESS PLAYER** (FAST, rule-compliant)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b01afc4",
      "metadata": {
        "id": "2b01afc4"
      },
      "source": [
        "The agent is implemented as a ReconChess Player, fully compliant with the game’s interface and rules.\n",
        "\n",
        "All game interactions—including sensing, move selection, belief updates, and board state tracking—are handled through the standard ReconChess callbacks. This ensures that the agent can play against other bots without relying on privileged information or modified game mechanics. The focus is on robustness and correct interaction rather than maximal performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class FastBotConfig:\n",
        "    sims: int = 80\n",
        "    c_puct: float = 1.5\n",
        "class RBCFastAZPlayer(Player):\n",
        "    def __init__(self, model: nn.Module, cfg: FastBotConfig, seed: int = 0):\n",
        "        self.model = model\n",
        "        self.cfg = cfg\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        self.color: Optional[chess.Color] = None\n",
        "        self.own_board: Optional[chess.Board] = None\n",
        "        self.B: Optional[torch.Tensor] = None\n",
        "        self.opp_counts: Optional[Dict[str,int]] = None\n",
        "        self.store_training = False\n",
        "        self.X: List[np.ndarray] = []\n",
        "        self.P: List[np.ndarray] = []\n",
        "        self.Z: List[float] = []\n",
        "    def handle_game_start(self, color: Color, board: chess.Board, opponent_name: str):\n",
        "        self.color = bool(color)\n",
        "        self.own_board = chess.Board(None)\n",
        "        self.own_board.clear()\n",
        "        for sq, pc in board.piece_map().items():\n",
        "            if pc.color == self.color:\n",
        "                self.own_board.set_piece_at(sq, pc)\n",
        "        self.own_board.turn = board.turn\n",
        "        self.own_board.castling_rights = board.castling_rights\n",
        "        self.B = init_belief_from_initial(self.color)\n",
        "        self.opp_counts = dict(START_COUNTS)\n",
        "    def handle_opponent_move_result(self, captured_my_piece: bool, capture_square: Optional[Square]):\n",
        "        if captured_my_piece and capture_square is not None:\n",
        "            if self.own_board is not None:\n",
        "                self.own_board.remove_piece_at(capture_square)\n",
        "    def choose_sense(self, sense_actions: List[Square], move_actions: List[chess.Move], seconds_left: float) -> Square:\n",
        "        assert self.B is not None\n",
        "        return choose_sense_square_entropy(self.B, sense_actions)\n",
        "    def handle_sense_result(self, sense_result: List[Tuple[Square, Optional[chess.Piece]]]):\n",
        "        assert self.B is not None and self.color is not None\n",
        "        self.B = apply_sense_to_belief(self.B, sense_result, self.color)\n",
        "    def choose_move(self, move_actions: List[chess.Move], seconds_left: float) -> Optional[chess.Move]:\n",
        "        assert self.own_board is not None and self.color is not None and self.B is not None and self.opp_counts is not None\n",
        "        if not move_actions:\n",
        "            return None\n",
        "        x_root = encode_state(self.own_board, self.color, self.B)\n",
        "        det = greedy_determinize_opponent(self.B, self.own_board, self.color, self.opp_counts)\n",
        "        det.turn = self.own_board.turn\n",
        "        N_a = puct_root(\n",
        "            model=self.model,\n",
        "            x_root=x_root,\n",
        "            det_board=det,\n",
        "            move_actions=move_actions,\n",
        "            sims=self.cfg.sims,\n",
        "            c_puct=self.cfg.c_puct,\n",
        "            device=DEVICE,\n",
        "        )\n",
        "        if self.store_training:\n",
        "            self.X.append(x_root.detach().cpu().numpy().astype(np.float32))\n",
        "            self.P.append(visits_to_policy_target(N_a))\n",
        "        if N_a:\n",
        "            best_a = max(N_a.items(), key=lambda kv: kv[1])[0]\n",
        "            mv = index_to_move(best_a)\n",
        "            if mv in move_actions:\n",
        "                return mv\n",
        "        return move_actions[int(self.rng.integers(0, len(move_actions)))]\n",
        "    def handle_move_result(\n",
        "        self,\n",
        "        requested_move: Optional[chess.Move],\n",
        "        taken_move: Optional[chess.Move],\n",
        "        captured_opponent_piece: bool,\n",
        "        capture_square: Optional[Square],\n",
        "    ):\n",
        "        if self.own_board is None or self.color is None:\n",
        "            return\n",
        "        if taken_move is not None:\n",
        "            apply_taken_move_to_own_board(self.own_board, taken_move, self.color)\n",
        "        if captured_opponent_piece and capture_square is not None and self.opp_counts is not None and self.B is not None:\n",
        "            r = chess.square_rank(capture_square)\n",
        "            f = chess.square_file(capture_square)\n",
        "            probs = self.B[:, r, f].detach().cpu()\n",
        "            best_sym = None\n",
        "            best_p = -1.0\n",
        "            for sym in PIECE_TYPES_6:\n",
        "                p = float(probs[CH2I[sym]].item())\n",
        "                if p > best_p and self.opp_counts.get(sym, 0) > 0:\n",
        "                    best_p = p\n",
        "                    best_sym = sym\n",
        "            if best_sym is None:\n",
        "                if self.opp_counts.get(\"P\", 0) > 0:\n",
        "                    best_sym = \"P\"\n",
        "                else:\n",
        "                    for sym in [\"N\", \"B\", \"R\", \"Q\", \"K\"]:\n",
        "                        if self.opp_counts.get(sym, 0) > 0:\n",
        "                            best_sym = sym\n",
        "                            break\n",
        "            if best_sym is not None:\n",
        "                self.opp_counts[best_sym] = max(0, self.opp_counts.get(best_sym, 0) - 1)\n",
        "    def handle_game_end(self, winner_color: Optional[Color], reason: str, game_history: Any):\n",
        "        if not self.store_training or self.color is None:\n",
        "            return\n",
        "        if winner_color is None:\n",
        "            z = 0.0\n",
        "        else:\n",
        "            z = 1.0 if bool(winner_color) == self.color else -1.0\n",
        "        self.Z = [z] * len(self.X)\n"
      ],
      "metadata": {
        "id": "S9T6sLD1S8qt"
      },
      "id": "S9T6sLD1S8qt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cd218048",
      "metadata": {
        "id": "cd218048"
      },
      "source": [
        "## LOCAL MATCH HARNESS (ReconChess) — smoke test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A local match harness is used to run short games against a baseline opponent, serving as an end-to-end smoke test for rule compliance and framework integration."
      ],
      "metadata": {
        "id": "atnVMf10-rtL"
      },
      "id": "atnVMf10-rtL"
    },
    {
      "cell_type": "code",
      "source": [
        "def try_discover_play_local_game():\n",
        "    try:\n",
        "        from reconchess.utilities import play_local_game\n",
        "        return play_local_game\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        from reconchess.scripts.rc_play_game import play_local_game\n",
        "        return play_local_game\n",
        "    except Exception:\n",
        "        pass\n",
        "    raise ImportError(\n",
        "        \"Could not find play_local_game in reconchess. Install a version that provides local runner utilities.\"\n",
        "    )\n",
        "def smoke_test_vs_random(n_games: int = 5, seed: int = 0):\n",
        "    play_local_game = try_discover_play_local_game()\n",
        "    model = FastPolicyValueNet(in_ch=15, trunk=64).to(DEVICE)\n",
        "    cfg = FastBotConfig(sims=40, c_puct=1.5)\n",
        "    wins = 0\n",
        "    losses = 0\n",
        "    draws = 0\n",
        "    for g in range(n_games):\n",
        "        bot = RBCFastAZPlayer(model=model, cfg=cfg, seed=seed + g)\n",
        "        opp = reconchess.bots.random_bot.RandomBot()\n",
        "        winner_color, reason, history = play_local_game(bot, opp)\n",
        "        if winner_color is None:\n",
        "            draws += 1\n",
        "        elif bool(winner_color) == chess.WHITE:\n",
        "            wins += 1\n",
        "        else:\n",
        "            losses += 1\n",
        "        print(f\"[game {g}] winner={winner_color} reason={reason}\")\n",
        "    print({\"wins\": wins, \"losses\": losses, \"draws\": draws})\n"
      ],
      "metadata": {
        "id": "V1u5NK9yNweV"
      },
      "id": "V1u5NK9yNweV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0c48c030",
      "metadata": {
        "id": "0c48c030"
      },
      "source": [
        "### Run smoke test (uncomment)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c7230b9",
      "metadata": {
        "id": "9c7230b9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "1e6791e3",
      "metadata": {
        "id": "1e6791e3"
      },
      "source": [
        "## RUN ALL CHECKS (fast sanity gate)\n",
        "These checks are meant to fail fast if something is inconsistent. If they pass, the agent is generally safe to run in local matches and self-play\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_all_checks():\n",
        "    model = FastPolicyValueNet(in_ch=15, trunk=64).to(DEVICE)\n",
        "    bot = RBCFastAZPlayer(model=model, cfg=FastBotConfig(sims=10, c_puct=1.5), seed=0)\n",
        "    start_board = chess.Board()\n",
        "    bot.handle_game_start(color=chess.WHITE, board=start_board, opponent_name=\"opp\")\n",
        "    assert bot.B is not None\n",
        "    s = bot.B.sum(dim=0)\n",
        "    assert float((s - 1.0).abs().max().item()) < 1e-4, \"Belief not normalized per square\"\n",
        "    sense_actions = list(range(64))\n",
        "    mv_actions = list(start_board.legal_moves)\n",
        "    sq = bot.choose_sense(sense_actions, mv_actions, seconds_left=100.0)\n",
        "    assert sq in sense_actions, \"choose_sense returned illegal square\"\n",
        "    mv = bot.choose_move(mv_actions, seconds_left=100.0)\n",
        "    assert (mv is None) or (mv in mv_actions), \"choose_move returned move not in move_actions\"\n",
        "    print(\"Core invariants: OK\")\n",
        "    try:\n",
        "        smoke_test_vs_random(n_games=2, seed=0)\n",
        "        print(\"Smoke games: OK\")\n",
        "    except Exception as e:\n",
        "        print(\"Smoke games: SKIPPED/FAILED (environment issue):\", e)\n",
        "run_all_checks()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tEHi89paukk",
        "outputId": "6872e34f-4637-4a20-ab11-c69d0007c900"
      },
      "id": "7tEHi89paukk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Core invariants: OK\n",
            "Smoke games: SKIPPED/FAILED (environment issue): Could not find play_local_game in reconchess. Install a version that provides local runner utilities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f18c59af",
      "metadata": {
        "id": "f18c59af"
      },
      "source": [
        "# SELF-PLAY + TRAINING (FAST)\n",
        "\n",
        "This section makes the bot trainable with minimal extra code:\n",
        "\n",
        "1) ReplayBuffer in RAM (FAST)\n",
        "2) Training step: KL(policy) + MSE(value)\n",
        "3) Self-play game generator (ReconChess local runner)\n",
        "4) Iterative loop: self-play → train → eval → checkpoint\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "986d1186",
      "metadata": {
        "id": "986d1186"
      },
      "source": [
        "## ReplayBuffer (RAM) + Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self-play generates training samples over time, so the implementation stores them in a replay buffer kept in RAM.\n",
        "\n",
        "The buffer collects tuples (X,π,z), where X is the encoded state, π is the search-improved policy target, and z is the final game outcome from the player’s perspective. A maximum capacity is enforced by discarding the oldest samples, keeping the dataset bounded and biased toward more recent experience.\n",
        "\n",
        "A lightweight PyTorch Dataset wrapper exposes the buffer in a format suitable for batching with a DataLoader, enabling standard supervised updates of the policy and value network."
      ],
      "metadata": {
        "id": "UjXDQ_MAA-DD"
      },
      "id": "UjXDQ_MAA-DD"
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size: int = 200_000):\n",
        "        self.max_size = max_size\n",
        "        self.X: List[np.ndarray] = []\n",
        "        self.P: List[np.ndarray] = []\n",
        "        self.Z: List[float] = []\n",
        "    def add(self, X: List[np.ndarray], P: List[np.ndarray], Z: List[float]):\n",
        "        assert len(X) == len(P) == len(Z)\n",
        "        self.X.extend(X)\n",
        "        self.P.extend(P)\n",
        "        self.Z.extend(Z)\n",
        "        if len(self.X) > self.max_size:\n",
        "            extra = len(self.X) - self.max_size\n",
        "            self.X = self.X[extra:]\n",
        "            self.P = self.P[extra:]\n",
        "            self.Z = self.Z[extra:]\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.X)\n",
        "class BufferDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, buf: ReplayBuffer):\n",
        "        self.buf = buf\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.buf)\n",
        "    def __getitem__(self, idx: int):\n",
        "        x = torch.from_numpy(self.buf.X[idx]).float()\n",
        "        p = torch.from_numpy(self.buf.P[idx]).float()\n",
        "        z = torch.tensor(self.buf.Z[idx], dtype=torch.float32)\n",
        "        return x, p, z\n"
      ],
      "metadata": {
        "id": "G3ouiU4EfeNL"
      },
      "id": "G3ouiU4EfeNL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5bb60965",
      "metadata": {
        "id": "5bb60965"
      },
      "source": [
        "## Training step (KL policy + MSE value)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Network parameters are updated using supervised learning on batches sampled from the replay buffer.\n",
        "\n",
        "The policy head is trained by minimizing the Kullback–Leibler divergence between the network’s predicted action distribution and the search-derived policy target. In parallel, the value head is trained using a mean squared error loss against the final game outcome.\n",
        "\n",
        "The two losses are combined into a single objective and optimized using standard gradient-based methods, with gradient clipping applied for stability."
      ],
      "metadata": {
        "id": "gGBXuu9pBqxu"
      },
      "id": "gGBXuu9pBqxu"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_steps(\n",
        "    model: nn.Module,\n",
        "    opt: torch.optim.Optimizer,\n",
        "    dl: torch.utils.data.DataLoader,\n",
        "    device: str,\n",
        "    steps: int = 200,\n",
        ") -> Dict[str, float]:\n",
        "    model.train()\n",
        "    it = iter(dl)\n",
        "    ema = {\"loss\": None, \"lp\": None, \"lv\": None}\n",
        "    for _ in range(steps):\n",
        "        try:\n",
        "            x, p, z = next(it)\n",
        "        except StopIteration:\n",
        "            it = iter(dl)\n",
        "            x, p, z = next(it)\n",
        "        x = x.to(device)\n",
        "        p = p.to(device)\n",
        "        z = z.to(device)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        logits, v_pred = model(x)\n",
        "        logp = F.log_softmax(logits, dim=-1)\n",
        "        loss_policy = F.kl_div(logp, p, reduction=\"batchmean\")\n",
        "        loss_value = F.mse_loss(v_pred, z)\n",
        "        loss = loss_policy + loss_value\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        opt.step()\n",
        "        for k, val in [(\"loss\", loss.item()), (\"lp\", loss_policy.item()), (\"lv\", loss_value.item())]:\n",
        "            if ema[k] is None:\n",
        "                ema[k] = float(val)\n",
        "            else:\n",
        "                ema[k] = 0.98 * ema[k] + 0.02 * float(val)\n",
        "    return {k: float(v) for k, v in ema.items()}\n"
      ],
      "metadata": {
        "id": "vpg9K1XDjNqB"
      },
      "id": "vpg9K1XDjNqB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1015be23",
      "metadata": {
        "id": "1015be23"
      },
      "source": [
        "## Checkpoint helpers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To support long-running experiments and allow training to be resumed across sessions, helper functions are provided to save and load model checkpoints.\n",
        "\n",
        "Each checkpoint stores the network parameters, optimizer state, and basic training metadata, ensuring that training can be restarted consistently without loss of information."
      ],
      "metadata": {
        "id": "CMT1C2IeCxj_"
      },
      "id": "CMT1C2IeCxj_"
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(\n",
        "    path: str,\n",
        "    model: nn.Module,\n",
        "    opt: torch.optim.Optimizer,\n",
        "    step: int,\n",
        "    extra: Optional[dict] = None\n",
        ") -> None:\n",
        "    ckpt = {\n",
        "        \"model\": model.state_dict(),\n",
        "        \"opt\": opt.state_dict(),\n",
        "        \"step\": int(step),\n",
        "        \"extra\": extra or {},\n",
        "    }\n",
        "    torch.save(ckpt, path)\n",
        "def load_checkpoint(\n",
        "    path: str,\n",
        "    model: nn.Module,\n",
        "    opt: Optional[torch.optim.Optimizer] = None\n",
        ") -> dict:\n",
        "    ckpt = torch.load(path, map_location=\"cpu\")\n",
        "    model.load_state_dict(ckpt[\"model\"])\n",
        "    if opt is not None and \"opt\" in ckpt:\n",
        "        opt.load_state_dict(ckpt[\"opt\"])\n",
        "    return ckpt\n"
      ],
      "metadata": {
        "id": "hOZMQeMloeqY"
      },
      "id": "hOZMQeMloeqY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58df40f2",
      "metadata": {
        "id": "58df40f2"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(path: str, model: nn.Module, opt: torch.optim.Optimizer, step: int, extra: Optional[dict] = None) -> None:\n",
        "    ckpt = {\"model\": model.state_dict(), \"opt\": opt.state_dict(), \"step\": int(step), \"extra\": extra or {}}\n",
        "    torch.save(ckpt, path)\n",
        "def load_checkpoint(path: str, model: nn.Module, opt: Optional[torch.optim.Optimizer] = None) -> dict:\n",
        "    ckpt = torch.load(path, map_location=\"cpu\")\n",
        "    model.load_state_dict(ckpt[\"model\"])\n",
        "    if opt is not None and \"opt\" in ckpt:\n",
        "        opt.load_state_dict(ckpt[\"opt\"])\n",
        "    return ckpt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d996c0e",
      "metadata": {
        "id": "1d996c0e"
      },
      "source": [
        "## One self-play game (ReconChess) → (X, P, Z)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A single self-play game is executed by running two instances of the same ReconChess player against each other using the local game runner.\n",
        "\n",
        "During the game, each player records training samples (X,π) at decision time, where X is the encoded state and π is derived from the search visit counts. After the game ends, the final outcome is converted into a value target z and assigned to all samples collected by each player.\n",
        "\n",
        "The resulting lists (X,P,Z) provide one complete episode of training data that can be appended to the replay buffer."
      ],
      "metadata": {
        "id": "DeyM690GEOG7"
      },
      "id": "DeyM690GEOG7"
    },
    {
      "cell_type": "code",
      "source": [
        "def selfplay_one_game(\n",
        "    model: nn.Module,\n",
        "    cfg: FastBotConfig,\n",
        "    seed: int = 0,\n",
        ") -> Tuple[List[np.ndarray], List[np.ndarray], List[float], Optional[Color], str]:\n",
        "    play_local_game = try_discover_play_local_game()\n",
        "    bot_w = RBCFastAZPlayer(model=model, cfg=cfg, seed=seed)\n",
        "    bot_b = RBCFastAZPlayer(model=model, cfg=cfg, seed=seed + 1)\n",
        "    bot_w.store_training = True\n",
        "    bot_b.store_training = True\n",
        "    winner_color, reason, history = play_local_game(bot_w, bot_b)\n",
        "    X = bot_w.X + bot_b.X\n",
        "    P = bot_w.P + bot_b.P\n",
        "    Z = bot_w.Z + bot_b.Z\n",
        "    if len(Z) != len(X):\n",
        "        if winner_color is None:\n",
        "            z_w = 0.0\n",
        "        else:\n",
        "            z_w = 1.0 if bool(winner_color) == chess.WHITE else -1.0\n",
        "        z_b = -z_w\n",
        "        Z = [z_w] * len(bot_w.X) + [z_b] * len(bot_b.X)\n",
        "    return X, P, Z, winner_color, reason\n"
      ],
      "metadata": {
        "id": "-NoD36Zmg6Er"
      },
      "id": "-NoD36Zmg6Er",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0c0b3093",
      "metadata": {
        "id": "0c0b3093"
      },
      "source": [
        "## Self-play → train → eval loop (FAST)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main training loop alternates between data generation and network updates.\n",
        "\n",
        "At each iteration, a small batch of self-play games is generated to produce new (X,π,z) samples, which are appended to the replay buffer. The policy/value network is then updated for a fixed number of gradient steps using mini-batches sampled from the buffer.\n",
        "\n",
        "After training, the current model is evaluated in a short match series against a simple baseline opponent to provide a quick progress signal, and a checkpoint plus a CSV log entry are saved for later inspection"
      ],
      "metadata": {
        "id": "bDqwYyy8Eyx1"
      },
      "id": "bDqwYyy8Eyx1"
    },
    {
      "cell_type": "code",
      "source": [
        "def run_selfplay_training(\n",
        "    iters: int = 5,\n",
        "    games_per_iter: int = 6,\n",
        "    train_steps_per_iter: int = 300,\n",
        "    batch_size: int = 64,\n",
        "    sims_selfplay: int = 40,\n",
        "    sims_eval: int = 20,\n",
        "    seed: int = 0,\n",
        "    out_dir: str = \"fast_checkpoints\",\n",
        "    results_csv: str = \"results.csv\",\n",
        "):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    results_path = os.path.join(out_dir, results_csv)\n",
        "    fieldnames = [\n",
        "        \"timestamp\", \"iter\", \"buffer_size\",\n",
        "        \"loss\", \"loss_policy\", \"loss_value\",\n",
        "        \"eval_wins\", \"eval_losses\", \"eval_draws\",\n",
        "        \"sims_selfplay\", \"sims_eval\",\n",
        "        \"games_per_iter\", \"train_steps_per_iter\", \"batch_size\",\n",
        "        \"ckpt_path\",\n",
        "    ]\n",
        "    if not os.path.exists(results_path):\n",
        "        with open(results_path, \"w\", newline=\"\") as f:\n",
        "            w = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "            w.writeheader()\n",
        "    model = FastPolicyValueNet(in_ch=15, trunk=64).to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "    buf = ReplayBuffer(max_size=200_000)\n",
        "    step = 0\n",
        "    for it in range(iters):\n",
        "        cfg = FastBotConfig(sims=sims_selfplay, c_puct=1.5)\n",
        "        for g in range(games_per_iter):\n",
        "            X, P, Z, winner, reason = selfplay_one_game(model, cfg, seed=seed + it * 1000 + g)\n",
        "            buf.add(X, P, Z)\n",
        "        ds = BufferDataset(buf)\n",
        "        dl = torch.utils.data.DataLoader(\n",
        "            ds, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True\n",
        "        )\n",
        "        metrics = train_steps(model, opt, dl, device=DEVICE, steps=train_steps_per_iter)\n",
        "        try:\n",
        "            wins = 0\n",
        "            losses = 0\n",
        "            draws = 0\n",
        "            play_local_game = try_discover_play_local_game()\n",
        "            for eg in range(6):\n",
        "                bot = RBCFastAZPlayer(\n",
        "                    model=model,\n",
        "                    cfg=FastBotConfig(sims=sims_eval, c_puct=1.5),\n",
        "                    seed=seed + 999 + it * 10 + eg,\n",
        "                )\n",
        "                opp = reconchess.bots.random_bot.RandomBot()\n",
        "                winner_color, reason, history = play_local_game(bot, opp)\n",
        "                if winner_color is None:\n",
        "                    draws += 1\n",
        "                elif bool(winner_color) == chess.WHITE:\n",
        "                    wins += 1\n",
        "                else:\n",
        "                    losses += 1\n",
        "            eval_res = {\"wins\": wins, \"losses\": losses, \"draws\": draws}\n",
        "        except Exception as e:\n",
        "            eval_res = {\"wins\": 0, \"losses\": 0, \"draws\": 0}\n",
        "            print(\"Eval skipped/failed:\", e)\n",
        "        ckpt_path = os.path.join(out_dir, f\"ckpt_iter_{it}.pt\")\n",
        "        save_checkpoint(\n",
        "            ckpt_path,\n",
        "            model,\n",
        "            opt,\n",
        "            step,\n",
        "            extra={\"iter\": it, \"buffer\": len(buf)},\n",
        "        )\n",
        "        print(f\"[iter {it}] buffer={len(buf)} train={metrics} eval6={eval_res} saved={ckpt_path}\")\n",
        "        row = {\n",
        "            \"timestamp\": datetime.datetime.now().isoformat(timespec=\"seconds\"),\n",
        "            \"iter\": it,\n",
        "            \"buffer_size\": len(buf),\n",
        "            \"loss\": metrics.get(\"loss\"),\n",
        "            \"loss_policy\": metrics.get(\"lp\"),\n",
        "            \"loss_value\": metrics.get(\"lv\"),\n",
        "            \"eval_wins\": eval_res.get(\"wins\", 0),\n",
        "            \"eval_losses\": eval_res.get(\"losses\", 0),\n",
        "            \"eval_draws\": eval_res.get(\"draws\", 0),\n",
        "            \"sims_selfplay\": sims_selfplay,\n",
        "            \"sims_eval\": sims_eval,\n",
        "            \"games_per_iter\": games_per_iter,\n",
        "            \"train_steps_per_iter\": train_steps_per_iter,\n",
        "            \"batch_size\": batch_size,\n",
        "            \"ckpt_path\": ckpt_path,\n",
        "        }\n",
        "        with open(results_path, \"a\", newline=\"\") as f:\n",
        "            w = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "            w.writerow(row)\n",
        "        step += 1\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "Oiiqgqxynsxw"
      },
      "id": "Oiiqgqxynsxw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2cfea0e2",
      "metadata": {
        "id": "2cfea0e2"
      },
      "source": [
        "### Run a small self-play training (start tiny)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95f0a1bc",
      "metadata": {
        "id": "95f0a1bc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "8e7e62a6",
      "metadata": {
        "id": "8e7e62a6"
      },
      "source": [
        "## Results plots\n",
        "After training, plot loss and winrate vs random.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1017e4c",
      "metadata": {
        "id": "a1017e4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "80862dce-55e2-4755-9c4b-4c56fd5b29bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Example:\\ndf = plot_results(\"fast_checkpoints/results.csv\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_results(csv_path: str):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    if len(df) == 0:\n",
        "        print(\"Empty CSV.\")\n",
        "        return df\n",
        "    plt.figure()\n",
        "    plt.plot(df[\"iter\"], df[\"loss\"])\n",
        "    plt.xlabel(\"iter\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.title(\"Training loss\")\n",
        "    plt.show()\n",
        "    plt.figure()\n",
        "    total = (df[\"eval_wins\"] + df[\"eval_losses\"] + df[\"eval_draws\"]).clip(lower=1)\n",
        "    winrate = df[\"eval_wins\"] / total\n",
        "    plt.plot(df[\"iter\"], winrate)\n",
        "    plt.xlabel(\"iter\")\n",
        "    plt.ylabel(\"winrate vs random (eval)\")\n",
        "    plt.title(\"Winrate vs RandomBot\")\n",
        "    plt.show()\n",
        "    return df\n",
        "'''Example:\n",
        "df = plot_results(\"fast_checkpoints/results.csv\")'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook provides a complete and executable reference implementation of a learning-based RBC agent, suitable for experimentation and further extensions."
      ],
      "metadata": {
        "id": "884Ab5AwHbKs"
      },
      "id": "884Ab5AwHbKs"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
